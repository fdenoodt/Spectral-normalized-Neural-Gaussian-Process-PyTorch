{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3625f473",
   "metadata": {},
   "source": [
    "# PyTorch Implementation of SNGP\n",
    "\n",
    "The core concepts of **Spectral-normalised Neural Gaussian Process** (SNGP) can be found in the original paper https://arxiv.org/abs/2006.10108. SNGP's main idea is to improve the ability of deep neural networks to retain distance information and to leverage this to determine the distance between test examples and the training data to improve predictive uncertainty estimates.\n",
    "This notebook roughly follows the outline of https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/understanding/sngp.ipynb but aims to implement all functionality shown here in PyTorch. https://github.com/alartum/sngp-pytorch is also an excellent implementation to consider and was read in determining how to organise my implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c094c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn.datasets\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bda11",
   "metadata": {},
   "source": [
    "Define visualization macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84271c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 140\n",
    "\n",
    "DEFAULT_X_RANGE = (-3.5, 3.5)\n",
    "DEFAULT_Y_RANGE = (-2.5, 2.5)\n",
    "DEFAULT_CMAP = colors.ListedColormap([\"#377eb8\", \"#ff7f00\"])\n",
    "DEFAULT_NORM = colors.Normalize(vmin=0, vmax=1,)\n",
    "DEFAULT_N_GRID = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d997a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Show logs\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e399f",
   "metadata": {},
   "source": [
    "## The two moon dataset\n",
    "Create the trainining and evaluation datasets from the [two moon dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d616106",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_training_data(sample_size=500):\n",
    "  \"\"\"Create two moon training dataset.\"\"\"\n",
    "  train_examples, train_labels = sklearn.datasets.make_moons(\n",
    "      n_samples=2 * sample_size, noise=0.1)\n",
    "\n",
    "  # Adjust data position slightly.\n",
    "  train_examples[train_labels == 0] += [-0.1, 0.2]\n",
    "  train_examples[train_labels == 1] += [0.1, -0.2]\n",
    "\n",
    "  return train_examples, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc675ce",
   "metadata": {},
   "source": [
    "Evaluate the model's predictive behavior over the entire 2D input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7ca6d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_testing_data(x_range=DEFAULT_X_RANGE, y_range=DEFAULT_Y_RANGE, n_grid=DEFAULT_N_GRID):\n",
    "  \"\"\"Create a mesh grid in 2D space.\"\"\"\n",
    "  # testing data (mesh grid over data space)\n",
    "  x = np.linspace(x_range[0], x_range[1], n_grid)\n",
    "  y = np.linspace(y_range[0], y_range[1], n_grid)\n",
    "  xv, yv = np.meshgrid(x, y)\n",
    "  return np.stack([xv.flatten(), yv.flatten()], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fc160",
   "metadata": {},
   "source": [
    "To evaluate model uncertainty, add an out-of-domain (OOD) dataset that belongs to a third class. The model never sees these OOD examples during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05337aee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_ood_data(sample_size=500, means=(2.5, -1.75), vars=(0.01, 0.01)):\n",
    "  return np.random.multivariate_normal(\n",
    "      means, cov=np.diag(vars), size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b5050",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the train, test and OOD datasets.\n",
    "train_examples, train_labels = make_training_data(\n",
    "    sample_size=500)\n",
    "test_examples = make_testing_data()\n",
    "ood_examples = make_ood_data(sample_size=500)\n",
    "\n",
    "# Visualize\n",
    "pos_examples = train_examples[train_labels == 0]\n",
    "neg_examples = train_examples[train_labels == 1]\n",
    "\n",
    "plt.figure(figsize=(7, 5.5))\n",
    "\n",
    "plt.scatter(pos_examples[:, 0], pos_examples[:, 1], c=\"#377eb8\", alpha=0.5)\n",
    "plt.scatter(neg_examples[:, 0], neg_examples[:, 1], c=\"#ff7f00\", alpha=0.5)\n",
    "plt.scatter(ood_examples[:, 0], ood_examples[:, 1], c=\"red\", alpha=0.1)\n",
    "\n",
    "plt.legend([\"Postive\", \"Negative\", \"Out-of-Domain\"])\n",
    "\n",
    "plt.ylim(DEFAULT_Y_RANGE)\n",
    "plt.xlim(DEFAULT_X_RANGE)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808d63a",
   "metadata": {},
   "source": [
    "Here the blue and orange represent the positive and negative classes, and the red represents the OOD data. A model that quantifies the uncertainty well is expected to be confident when close to training data  (i.e., $p(x_{test})$ close to 0 or 1), and be uncertain when far away from the training data regions  (i.e., $p(x_{test})$ close to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656074e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "  def __init__(self, X, y, dtype_X = torch.FloatTensor, dtype_y = torch.LongTensor):\n",
    "    self.train_examples = torch.from_numpy(X).type(dtype_X)\n",
    "    self.train_labels = torch.from_numpy(y).type(dtype_y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.train_examples)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.train_examples[idx], self.train_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69476c5d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "logger.info(f\"{train_examples.shape=}, {train_labels.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7c605",
   "metadata": {},
   "source": [
    "### Visualize uncertainty\n",
    "\n",
    "The following code for plotting the classification and uncertainty surface can be found at https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/understanding/sngp.ipynb#scrollTo=HZDMX7gZrZ-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971391ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_uncertainty_surface(test_uncertainty, ax, cmap=None):\n",
    "  \"\"\"Visualizes the 2D uncertainty surface.\n",
    "\n",
    "  For simplicity, assume these objects already exist in the memory:\n",
    "\n",
    "    test_examples: Array of test examples, shape (num_test, 2).\n",
    "    train_labels: Array of train labels, shape (num_train, ).\n",
    "    train_examples: Array of train examples, shape (num_train, 2).\n",
    "\n",
    "  Arguments:\n",
    "    test_uncertainty: Array of uncertainty scores, shape (num_test,).\n",
    "    ax: A matplotlib Axes object that specifies a matplotlib figure.\n",
    "    cmap: A matplotlib colormap object specifying the palette of the\n",
    "      predictive surface.\n",
    "\n",
    "  Returns:\n",
    "    pcm: A matplotlib PathCollection object that contains the palette\n",
    "      information of the uncertainty plot.\n",
    "  \"\"\"\n",
    "  # Normalize uncertainty for better visualization.\n",
    "  test_uncertainty = test_uncertainty / np.max(test_uncertainty)\n",
    "\n",
    "  # Set view limits.\n",
    "  ax.set_ylim(DEFAULT_Y_RANGE)\n",
    "  ax.set_xlim(DEFAULT_X_RANGE)\n",
    "\n",
    "  # Plot normalized uncertainty surface.\n",
    "  pcm = ax.imshow(\n",
    "      np.reshape(test_uncertainty, [DEFAULT_N_GRID, DEFAULT_N_GRID]),\n",
    "      cmap=cmap,\n",
    "      origin=\"lower\",\n",
    "      extent=DEFAULT_X_RANGE + DEFAULT_Y_RANGE,\n",
    "      vmin=DEFAULT_NORM.vmin,\n",
    "      vmax=DEFAULT_NORM.vmax,\n",
    "      interpolation='bicubic',\n",
    "      aspect='auto')\n",
    "\n",
    "  # Plot training data.\n",
    "  ax.scatter(train_examples[:, 0], train_examples[:, 1],\n",
    "             c=train_labels, cmap=DEFAULT_CMAP, alpha=0.5)\n",
    "  ax.scatter(ood_examples[:, 0], ood_examples[:, 1], c=\"red\", alpha=0.1)\n",
    "\n",
    "  return pcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407b140",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_predictions(pred_probs, uncertainty, model_name=\"\"):\n",
    "  \"\"\"Plot normalized class probabilities and predictive uncertainties.\"\"\"\n",
    "  # Compute predictive uncertainty.\n",
    "\n",
    "  # Initialize the plot axes.\n",
    "  fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "  # Plots the class probability.\n",
    "  pcm_0 = plot_uncertainty_surface(pred_probs, ax=axs[0])\n",
    "  # Plots the predictive uncertainty.\n",
    "  pcm_1 = plot_uncertainty_surface(uncertainty, ax=axs[1])\n",
    "\n",
    "  # Adds color bars and titles.\n",
    "  fig.colorbar(pcm_0, ax=axs[0])\n",
    "  fig.colorbar(pcm_1, ax=axs[1])\n",
    "\n",
    "  axs[0].set_title(f\"Class Probability, {model_name}\")\n",
    "  axs[1].set_title(f\"(Normalized) Predictive Uncertainty, {model_name}\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1283f81",
   "metadata": {},
   "source": [
    "## The SNGP model\n",
    "\n",
    "The SNGP model should preserve distance from the training data and we can leverage this for better predictive uncertainty estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbf929",
   "metadata": {},
   "source": [
    "#### The ResNet Backbone\n",
    "We define a basic neural network with skip connections below. Spectral normalisation is implemented in the spectral_norm_hook which is called after every backprop which updates the weights in a layer, and this enforces the spectral norm of the layer weights to be constrained.\n",
    "\n",
    "Applying spectral normalisation to a value below 1.0, due to the Lipschitz condition (this is described in the SNGP paper in detail), distance in the hidden layer representation of the input bears correspondence to distance in the data manifold, making it possible to use distance in the hidden representation to produce predictive certainty estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb98c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import ResNetBackbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e92b0",
   "metadata": {},
   "source": [
    "#### Gaussian Process Output Layer\n",
    "As discussed in the SNGP paper, a Gaussian Process with a radial basis kernel is approximated using Random Fourier Features to approximate the prior distribution for the GP and then using the Laplace method to approximate the GP posterior. This yields a set of layers that are end to end trainable with the neural network backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ce3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gaussian_process_layer import RandomFeatureGaussianProcess\n",
    "from models.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "sngp_config = dict(\n",
    "        out_features=2,\n",
    "        backbone = ResNetBackbone(input_features=2, num_hidden_layers=5, num_hidden=128, dropout_rate=0.1),\n",
    "        num_inducing = 1024,\n",
    "        momentum = -1.0,\n",
    "        ridge_penalty = 1e-6)\n",
    "training_config = dict(batch_size=128, shuffle=True)\n",
    "trainer = Trainer(model_config=sngp_config, task_type='classification', model=RandomFeatureGaussianProcess, device=torch.device('cpu'))\n",
    "*_, model = trainer.train(training_data=TrainDataset(X=train_examples, y=train_labels), data_loader_config=training_config, epochs=60)\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cb24c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss(\"Two Moons Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07ab9d",
   "metadata": {},
   "source": [
    "#### Computing the posterior predictive probability\n",
    "We could use sampling but this requires multiple inferences and removes the latency benefits of SNGP over using ensembles or MC dropout to estimate predictive uncertainty. Instead, we approximate $E(p(x))$ using the mean-field method:\n",
    "\n",
    "$E(p(x)) \\approx softmax(\\frac{logit(x)}{\\sqrt(1 + \\lambda \\sigma^2(x))})$\n",
    "\n",
    "where often $\\lambda$ is chosen as $\\pi/8$ or $3/\\pi^2$.\n",
    "\n",
    "This has the benefit that we require one forward pass to produce the mean output prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6295c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mean_field_logits(logits, variances, mean_field_factor):\n",
    "  logits_scale = (1.0 + variances * mean_field_factor) ** 0.5\n",
    "  if len(logits.shape) > 1:\n",
    "    logits_scale = logits_scale[:, None]\n",
    "  return logits/logits_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc179fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.Tensor(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.update_covariance()\n",
    "logits, variances = model(test, with_variance=True, update_precision=False)\n",
    "logits = mean_field_logits(logits, variances, np.pi/8.)\n",
    "probs = logits.detach().softmax(dim=1).numpy()[:,0]\n",
    "variances = variances.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadf378",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(probs, variances, model_name=\"SNGP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that spectral normalisation has been applied to backbone layer weights\n",
    "logger.info([torch.linalg.norm(hidden_layer.weight, 2).detach().squeeze() for hidden_layer in model.rff[0].hidden_layers])\n",
    "logger.info(torch.linalg.norm(model.rff[0].input_layer.weight.detach(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5499b",
   "metadata": {},
   "source": [
    "# Regression\n",
    "We adapt training of the SNGP for regression tasks by updating the loss function used. As the Laplace method approximates the RFF posterior using a Gaussian likelihood centered around a MAP estimate, and minimising the negative log likelihood can be done by minimising the mean squared error, we can plug in the mean square error loss for the cross entropy loss in the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32cc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(start=-10000, stop=10000, num=100000).reshape(-1, 1)\n",
    "y = np.squeeze(X**3 + X - np.log(X**2))\n",
    "y = (y - np.mean(y)) / np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fc7cf",
   "metadata": {},
   "source": [
    "# The underlying function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa707d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[::1000], y[::1000], label=r\"$f(X) = X^3 + X - \\log(X^2)$\", linestyle=\"dotted\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"True generative process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4066108",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(y.size//4, y.size*3//4), size=4000, replace=False)\n",
    "# training_indices = np.arange(y.size//4, y.size*3//4)[::5]\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "noise_std = 0.01\n",
    "y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)\n",
    "train_dataset = TrainDataset(X_train, np.expand_dims(y_train_noisy, axis=1), torch.FloatTensor, torch.FloatTensor)\n",
    "logger.info(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc83e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sngp_config = dict(\n",
    "        out_features=1,\n",
    "        backbone = ResNetBackbone(input_features=1, num_hidden_layers=0, num_hidden=64, dropout_rate=0.01, norm_multiplier=0.9),\n",
    "        num_inducing = 1024,\n",
    "        momentum = -1.0,\n",
    "        ridge_penalty = 1e-6)\n",
    "\n",
    "# Note that for regression tasks especially, the spectral normalisation applies a limit on the size gradients can grow to.\n",
    "# This means that standardising and destandardising input and output data is especially useful in this context.\n",
    "training_config = dict(batch_size=32, shuffle=True)\n",
    "trainer = Trainer(model_config=sngp_config, task_type='regression', model=RandomFeatureGaussianProcess, device=torch.device('cpu'))\n",
    "*_, model = trainer.train(training_data=train_dataset, data_loader_config=training_config, epochs=100, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_loss(\"Basic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad02088",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21732d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.update_covariance()\n",
    "predictions, variances = model(test_dataset, with_variance=True, update_precision=False)\n",
    "predictions = predictions.detach().numpy()\n",
    "variances = variances.numpy()\n",
    "variances = variances/np.max(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, variances, linestyle='dotted', label='Standardised Variance')\n",
    "plt.plot(X[:-999], np.convolve(variances, np.ones(1000)/1000, mode='valid'), linestyle='dotted', label='Moving average')\n",
    "plt.scatter(X_train, np.zeros(len(X_train)), s=1, c='red', label='Support of training data')\n",
    "plt.legend()\n",
    "plt.ylabel('Standardised variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0be2527",
   "metadata": {},
   "source": [
    "## What this shows\n",
    "These plots illustrate how the variance output by the network grows with distance from the training data manifold, capturing a notion of uncertainty quantified by distance from training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0234ff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.plot(X[::500], predictions[::500], label=r\"Predictions, variance, and true training data\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train_noisy, s=1, c='red', label='In Distribution Training Samples')\n",
    "plt.fill_between(X[::1000].transpose()[0], predictions[::1000].transpose()[0]-variances[::1000].transpose()[0], predictions[::1000].transpose()[0]+variances[::1000].transpose()[0], alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0882c5",
   "metadata": {},
   "source": [
    "## What this shows\n",
    "The function is relatively easy to model but it illustrates how the variability of predictions is far greater outside the support of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a46b07",
   "metadata": {},
   "source": [
    "### 2D Input Data\n",
    "Here I will try and illustrate this distance related out of distribution uncertainty increase that is seen, but with a 2D input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa904a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X, Y):\n",
    "  return np.sin(X) + np.cos(Y)\n",
    "\n",
    "def f1(xs):\n",
    "  return xs[0] + np.sin(xs[1])\n",
    "\n",
    "X1 = np.arange(-10, 10, 0.01)\n",
    "Y1 = np.arange(-10, 10, 0.01)\n",
    "X, Y = np.meshgrid(X1, Y1)\n",
    "R = np.sqrt(X**2 + Y**2)\n",
    "Z = np.sin(R)\n",
    "Z = (Z-np.mean(Z, axis=1))/np.std(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79105fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.set_title('True Data Distribution')\n",
    "ax.plot_surface(X, Y, Z, linewidth=0, antialiased=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = np.array([[x, y] for x in X1 for y in Y1])\n",
    "train_labels = np.sin(np.sqrt(train_input[:, 0]**2 + train_input[:, 1]**2))\n",
    "logger.info(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf7f25",
   "metadata": {},
   "source": [
    "### Partition half the data for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(0, train_labels.shape[0]//2), size=80000, replace=False)\n",
    "X_train, y_train = train_input[training_indices], train_labels[training_indices]\n",
    "noise_std = 0.01\n",
    "y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(X_train, np.expand_dims(y_train_noisy, axis=1), torch.FloatTensor, torch.FloatTensor)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.Tensor(train_input[::10])\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002015c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "sngp_config = dict(\n",
    "        out_features=1,\n",
    "        backbone = ResNetBackbone(input_features=2, num_hidden_layers=1, num_hidden=128, dropout_rate=0.01, norm_multiplier=0.9),\n",
    "        num_inducing = 1024,\n",
    "        momentum = -1.0,\n",
    "        ridge_penalty = 1e-6)\n",
    "\n",
    "# again, should ideally standardise inputs and outputs (ignored here as domain and range are both relatively non large in magnitude)\n",
    "training_config = dict(batch_size=128, shuffle=True)\n",
    "trainer = Trainer(model_config=sngp_config, task_type='regression', model=RandomFeatureGaussianProcess, device=torch.device('cpu'))\n",
    "*_, model = trainer.train(training_data=train_dataset, data_loader_config=training_config, epochs=30, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04651fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_loss(\"3D function regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d922ffb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.reset_covariance()\n",
    "model.update_covariance()\n",
    "predictions, variances = model(test_dataset, with_variance=True, update_precision=False)\n",
    "predictions = predictions.detach().numpy()\n",
    "variances = variances.abs().numpy()\n",
    "variances = variances/np.max(variances)\n",
    "logger.info(f\"{variances.shape=}, {predictions.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90bd05f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# credit: https://stackoverflow.com/questions/30715083/python-plotting-a-wireframe-3d-cuboid\n",
    "def cuboid_data(center, size):\n",
    "    # suppose axis direction: x: to left; y: to inside; z: to upper\n",
    "    # get the (left, outside, bottom) point\n",
    "    o = [a - b / 2 for a, b in zip(center, size)]\n",
    "    # get the length, width, and height\n",
    "    l, w, h = size\n",
    "    x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in bottom surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in upper surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in outside surface\n",
    "         [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  # x coordinate of points in inside surface\n",
    "    y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  # y coordinate of points in bottom surface\n",
    "         [o[1], o[1], o[1] + w, o[1] + w, o[1]],  # y coordinate of points in upper surface\n",
    "         [o[1], o[1], o[1], o[1], o[1]],          # y coordinate of points in outside surface\n",
    "         [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]    # y coordinate of points in inside surface\n",
    "    z = [[o[2], o[2], o[2], o[2], o[2]],                        # z coordinate of points in bottom surface\n",
    "         [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],    # z coordinate of points in upper surface\n",
    "         [o[2], o[2], o[2] + h, o[2] + h, o[2]],                # z coordinate of points in outside surface\n",
    "         [o[2], o[2], o[2] + h, o[2] + h, o[2]]]                # z coordinate of points in inside surface\n",
    "    return np.array(x), np.array(y), np.array(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.scatter3D(train_input[:,0][::10], train_input[:,1][::10], predictions[:,0], c=predictions[:,0], label='Predictions')\n",
    "X, Y, Z = cuboid_data([-5, 0, 0], (10, 20, 20))\n",
    "surf = ax.plot_surface(X, Y, Z, color='b', rstride=1, cstride=1, alpha=0.1, label='Training data support region')\n",
    "surf._facecolors2d = surf._facecolor3d\n",
    "surf._edgecolors2d = surf._edgecolor3d\n",
    "ax.set_title('Predictions')\n",
    "ax.set_xlabel('sample element [0]')\n",
    "ax.set_ylabel('sample element [1]')\n",
    "ax.set_zlabel('Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311241b8",
   "metadata": {},
   "source": [
    "The following plot illustrates how uncertainty increases sharply as we move from in distribution to out of distribution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "X, Y, Z = cuboid_data([-5, 0, 0.5], (10, 20, 0.9))\n",
    "surf = ax.plot_surface(X, Y, Z, color='b', rstride=1, cstride=1, alpha=0.1, label='Training data support')\n",
    "surf._facecolors2d = surf._facecolor3d\n",
    "surf._edgecolors2d = surf._edgecolor3d\n",
    "\n",
    "ax.scatter3D(train_input[:,0][::10], train_input[:,1][::10], variances, c=variances, label='Predictive uncertainty')\n",
    "ax.set_xlabel('sample element [0]')\n",
    "ax.set_ylabel('sample element [1]')\n",
    "ax.set_zlabel('Scaled Uncertainty')\n",
    "ax._facecolors2d = ax._facecolor\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
